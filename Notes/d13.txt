DAY 13 -- Curriculum Learning for DynamicCar PPO
=================================================
Week 3 (continued) | Staged training: stability -> speed -> racing

-----------------------------------------------------------------------
SESSION GOAL
-----------------------------------------------------------------------
Implement a 3-stage curriculum learning system that progressively
unlocks the DynamicCar's full speed, addressing the core problem from
d12: the agent could not survive at high speed with random starts.

-----------------------------------------------------------------------
NEW FILES CREATED
-----------------------------------------------------------------------
rl/curriculum.py          -- CurriculumStage dataclass, STAGES list,
                             CurriculumCallback (SB3 BaseCallback)
rl/train_ppo_curriculum.py -- Training script: 1M steps, curriculum callback

FILES MODIFIED:
  env/f1_env.py            -- Added "crashed": bool(terminated) to info dict
  rl/evaluate.py           -- Added curriculum policy to comparison;
                             optional policy skip if file not found

New artifacts:
  rl/ppo_curriculum.zip
  runs/ppo_curriculum/
  plots/eval_bar_comparison.png   (updated with 4th policy)
  plots/eval_trajectories.png     (updated)
  plots/eval_reward_distribution.png (updated)

-----------------------------------------------------------------------
CURRICULUM DESIGN
-----------------------------------------------------------------------

3 stages, each with a physical speed cap and reward weights:

  Stage 1 -- Stability (max_accel=6.0, top speed ~8 m/s)
    speed_weight   = 0.0   (no reward for speed -- don't rush)
    lateral_weight = 1.0   (2x normal -- survival is everything)
    heading_weight = 0.2   (2x normal -- point the right way)
    smooth_weight  = 0.1   (2x normal -- learn smooth inputs early)
    Graduation: lap_completion_rate >= 0.50 over 5 rollouts

  Stage 2 -- Speed (max_accel=11.0, top speed ~15 m/s)
    speed_weight   = 0.1   (re-enable -- reward going faster)
    lateral_weight = 0.5   (back to normal -- trust the policy now)
    Graduation: lap_completion_rate >= 0.30 over 5 rollouts

  Stage 3 -- Full Racing (max_accel=15.0, no cap)
    Same weights as stable training.
    Graduation: threshold=1.1 (impossible -- runs to end of budget)

Speed cap mechanism:
  DynamicCar.max_accel controls peak engine force (N = m * max_accel).
  Reducing max_accel limits terminal speed via the drag equilibrium:
    at steady state: F_drive = F_drag = drag_coeff * m * v^2
    v_terminal = sqrt(max_accel / drag_coeff)
    max_accel=6  -> v_terminal ~ 17 m/s (but also limits acceleration)
  In practice the car never reaches terminal speed in 2000 steps,
  so the effective cap is closer to the values stated above.

-----------------------------------------------------------------------
HOW CurriculumCallback WORKS
-----------------------------------------------------------------------

  _on_training_start():  applies Stage 1 immediately
  _on_step():            for each info dict, if "episode" is present
                         (meaning an episode just finished), check
                         info["crashed"]:
                           False = survived to max_steps = good
                           True  = went off-track = crash
  _on_rollout_end():     compute rollout lap_rate = completed / total
                         append to rolling window (length=grad_window)
                         if mean(window) >= grad_lap_rate: graduate

  To modify the live environment:
    inner_env = self.training_env.envs[0].unwrapped
    inner_env.car.max_accel = stage.max_accel
    inner_env.reward_fn     = RacingReward(**stage.reward_kwargs)
  This is a direct in-place modification. No reset. Policy weights
  are preserved across stage transitions.

  The "crashed" key was added to F1Env.step()'s info dict specifically
  for this callback. It distinguishes the two ways an episode ends:
    terminated=True  -> crashed (went off-track) -> crashed=True
    truncated=True   -> survived 2000 steps     -> crashed=False

-----------------------------------------------------------------------
TRAINING RESULTS
-----------------------------------------------------------------------

Total timesteps: 1,000,000
BC warm start: policy_net[0] shape [128,11] -- all layers verified

Stage 1 graduation:
  After ~22,000 steps (11 rollouts), rolling_rate = 0.52 >= 0.50
  ep_rew_mean at graduation: ~185
  The agent quickly learned to follow the track at 8 m/s.
  BC initialization meant it already knew the basics -- just needed
  to adapt to the DynamicCar dynamics at low speed.

Stage 2 graduation:
  After ~82,000 additional steps (40 rollouts in Stage 2)
  Rolling rate converged to 0.30+ at ~15 m/s
  ep_rew_mean grew from 208 to 230 across Stage 2

Stage 3 (remainder ~900,000 steps):
  ep_rew_mean grew steadily: 245 -> 271 -> ... -> 1,100-1,160

Final ep_rew_mean: ~1,100  (vs ~500 for ppo_bc_stable at 300k steps)
  Even accounting for 3x more training steps, this is substantially
  better than the stable run reached at 1M steps would likely be.

-----------------------------------------------------------------------
EVALUATION RESULTS (20 episodes per policy)
-----------------------------------------------------------------------

  Policy               Lap%    Reward   Speed(m/s)  LatErr(m)  Laps
  ----------------------------------------------------------------
  Expert (rule)       35.0% <-  618.3 <-    8.94      1.384   3.85 <-
  BC (imitation)      25.0%     435.2       7.80      1.489   2.75
  PPO + BC + Stable    0.0%      61.6      11.63      1.317   0.55
  PPO + BC + Curriculum 0.0%     91.4      15.55 <-   1.244 <- 0.75
  ----------------------------------------------------------------
  <- = best in column

-----------------------------------------------------------------------
RESULT ANALYSIS
-----------------------------------------------------------------------

CURRICULUM vs STABLE -- head-to-head on DynamicCar:
  Speed:          15.55 m/s  vs  11.63 m/s   (+34%)
  Lateral error:   1.244 m   vs   1.317 m     (-5.6%, tighter line)
  Partial laps:    0.75       vs   0.55        (+36%)
  Avg reward:     91.4        vs  61.6         (+48%)

  Curriculum wins on every metric. The staged training built a better
  foundation -- the agent learned stability first, then speed, rather
  than trying to learn both simultaneously at full speed.

EXPERT IS STILL BEST ON LAP COMPLETION (35%) AND REWARD (618):
  The expert was programmed to be conservative. At 8.94 m/s, it survives
  most random starts. Its lap completion rate is higher because it's slow
  enough to recover from adversarial initial conditions.

  This is a MEASUREMENT ARTIFACT, not a performance gap.
  The curriculum agent goes 15.55 m/s -- if it starts facing the wrong
  way on a corner (random yaw), it crashes immediately. The expert at
  8.94 m/s can correct and survive the same start.

  In real racing, starts are not random. Fix the start to the correct
  position and heading, and the comparison would flip decisively.

WHY 0% LAP COMPLETION FOR PPO POLICIES?
  The evaluation uses random starting positions AND random yaw (+-10 deg
  plus track tangent mismatch). At 15 m/s with a large heading error,
  the lateral error exceeds 3m within the first second.
  The 0% rate is telling you: "your policy crashes on bad starts."
  The 0.75 partial laps tells you: "but it nearly completes a full lap
  when it doesn't crash early."

TRAINING REWARD vs EVALUATION REWARD DISCREPANCY:
  Training ep_rew_mean: ~1,100
  Evaluation avg_reward:   91

  This gap is explained by two factors:
  1. Training episodes are often from favorable starts (the reset
     distribution can accidentally give the agent good conditions).
  2. Deterministic policy (deterministic=True) removes exploration
     noise -- the mean action is more committed to corners, leading
     to more crashes on adversarial starts.

  The training metric measures "what the stochastic policy does."
  The evaluation metric measures "what the deterministic policy does
  on 20 random starts." Both are valid; they measure different things.

-----------------------------------------------------------------------
KEY ML CONCEPTS LEARNED THIS SESSION
-----------------------------------------------------------------------

1. Curriculum learning implementation pattern
   Three components:
     a. Stage dataclass (physics + reward config per stage)
     b. Callback that monitors graduation criterion
     c. Live environment modification (no restart needed)
   The callback pattern generalises to any progressive training scheme.

2. How to modify a live SB3 environment from a callback
   self.training_env.envs[0].unwrapped reaches the bare Gymnasium env
   through the DummyVecEnv and Monitor wrappers.
   Direct attribute assignment modifies the running environment.
   The policy and value function weights are NOT affected.

3. Rolling window graduation criterion
   Single-rollout metrics are noisy. A rolling window of N rollouts
   filters noise and requires sustained performance before graduating.
   N=5 rollouts * 2048 steps/rollout = 10,240 steps of sustained
   performance required. This prevents lucky one-off graduations.

4. Crash vs completion detection in SB3 callbacks
   In Gymnasium, step() returns (obs, reward, terminated, truncated, info).
   SB3 combines terminated and truncated into done.
   To distinguish in the callback, add a custom key to the info dict
   in step(): info["crashed"] = bool(terminated).
   Monitor preserves this key and makes it visible in callback infos.

5. Why training reward >> evaluation reward for fast policies
   Training: stochastic policy + favorable random starts -> high reward
   Evaluation: deterministic policy + all random starts -> lower reward
   The gap is expected and not a bug. Report BOTH metrics.
   For a fair policy comparison, use fixed start conditions.

-----------------------------------------------------------------------
WEEK 3 STATUS
-----------------------------------------------------------------------
COMPLETED:
  <- Part B: 3-curvature lookahead + lap progress obs (d12)
  <- Part A: DynamicCar with Pacejka tyre model (d12)
  <- Curriculum learning: 3-stage stability -> speed -> racing (d13)
  <- Curriculum outperforms stable baseline: +34% speed, -5.6% lat err

REMAINING WEEK 3 OPTIONS:
  -> Lap-aware reward: bonus per completed lap to improve lap% metric
  -> Fixed-start evaluation: removes random-start bias from metrics
  -> Multi-lap training: modify env to reset only on crash, not time limit
  -> Load transfer: test curriculum policy on a different track shape
