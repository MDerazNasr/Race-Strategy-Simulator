DAY 14 -- Lap-Aware Reward + Fixed-Start Evaluation
=====================================================
Week 4 (start) | Cleaning up metrics; making the evaluation tell the truth

-----------------------------------------------------------------------
SESSION GOAL
-----------------------------------------------------------------------
Two targeted improvements from the "what's next" list in PROGRESS.txt:

  1. Lap-aware reward: add a +100 bonus every time the agent completes
     a full lap. The curriculum agent averaged 0.75 partial laps but showed
     0% lap-completion rate — a lap bonus gives it an explicit incentive to
     stay alive to the end of a lap.

  2. Fixed-start evaluation: run all policies from the same starting
     position (waypoint 0, track-aligned, v=5 m/s). The random-start
     benchmark was penalising fast policies unfairly: at 15–25 m/s a bad
     starting heading causes a crash in < 1 second, while the slow expert
     (8 m/s) can correct the same error and survive.

Neither change retrains any model. Both are purely about making the
environment signal and the evaluation metric more honest.

-----------------------------------------------------------------------
FILES CHANGED
-----------------------------------------------------------------------
env/f1_env.py     -- lap tracking attributes, fixed_start in reset(),
                     lap detection in step(), lap_bonus added to reward,
                     laps_completed added to info dict.

rl/evaluate.py    -- fixed_start param threaded through run_episode /
                     run_episodes / evaluate_all; print_comparison_table
                     gets a title param; plot_bar_comparison gets a
                     subtitle param; main block runs BOTH evaluation modes
                     and saves two bar charts.

New artifacts:
  plots/eval_bar_comparison.png          (random start — updated)
  plots/eval_bar_comparison_fixed.png    (fixed start — NEW)
  plots/eval_trajectories.png            (updated)
  plots/eval_reward_distribution.png     (updated)

-----------------------------------------------------------------------
PART 1 — LAP-AWARE REWARD
-----------------------------------------------------------------------

Design:
  The oval track has 200 waypoints (indices 0–199).
  A lap completes when the car's closest waypoint index jumps from
  a high value (> 150) back to a low value (< 50): it has crossed
  the start/finish line (waypoint 0) in the forward direction.

  This is the SAME wrap-around detection that evaluate.py previously
  computed locally using closest_point(). Moving it into the env:
    a) Gives one source of truth — evaluate.py now reads env.laps_completed
       instead of recomputing it.
    b) Makes the signal available to the agent during TRAINING, not just
       evaluation. Without this the agent had no idea laps existed.

Implementation in env/f1_env.py:
  __init__:
    self.lap_bonus          = 100.0   # reward per lap
    self._track_idx         = 0       # set by get_obs() each call
    self._prev_track_idx    = 0       # previous step's index
    self.laps_completed     = 0       # episode cumulative count

  get_obs():
    idx, _ = closest_point(...)
    self._track_idx = idx             # expose for step()

  reset():
    self.laps_completed  = 0
    ...
    obs = self.get_obs()              # sets self._track_idx
    self._prev_track_idx = self._track_idx   # init to actual start position
    # (prevents a spurious lap count on step 1 if starting near waypoint 0)

  step():
    obs = self.get_obs()              # updates self._track_idx
    curr_idx = self._track_idx
    if self._prev_track_idx > 150 and curr_idx < 50:
        self.laps_completed += 1
        lap_bonus = self.lap_bonus    # +100.0
    else:
        lap_bonus = 0.0
    self._prev_track_idx = curr_idx
    reward = self.reward_fn.compute(...) + lap_bonus
    info["laps_completed"] = self.laps_completed

Why +100.0?
  At maximum speed, one lap = ~180 steps × ~1.0 max per-step reward = ~180.
  100 is ~55% of a full-speed lap. This is large enough that:
    - The agent is rewarded decisively for lap completion.
    - Crashing right before the line is worse than crossing it.
  But not so large it overwhelms the per-step progress signal.

Why add AFTER reward_fn?
  reward_fn.compute() handles per-step shaping (progress, lateral penalty,
  heading penalty, smoothness, terminal penalty). The lap bonus is a discrete
  event bonus, structurally different from per-step shaping. Keeping them
  separate makes both easier to tune independently.

-----------------------------------------------------------------------
PART 2 — FIXED-START EVALUATION
-----------------------------------------------------------------------

The problem with random-start evaluation:
  F1Env.reset() samples a random track position and a random heading
  offset (±10°). This is the correct TRAINING distribution — teaching
  the agent to handle all starting conditions.

  But it's a biased EVALUATION distribution:
    - A fast policy (25 m/s) with a 10° heading error takes ~0.4 s to
      reach the track boundary (3 m). It crashes before the first corner.
    - The rule-based expert (8 m/s) takes ~1.3 s — enough time to
      correct its heading and survive.
    - Result: the expert scores higher on lap-completion rate than the
      faster PPO agent purely because of starting-condition luck.

  Fixed start removes this confound.

Implementation:
  F1Env.reset() now accepts options={"fixed_start": True}.
  When set:
    - start = self.track[0]        → waypoint 0 = (50, 0) on the oval
    - yaw   = track_tangent(...)   → aligned with the track at wpt 0
    - v     = 5.0                  → moderate, consistent starting speed
  No random perturbation.

  run_episode() in evaluate.py:
    reset_opts = {"fixed_start": True} if fixed_start else None
    obs, info = env.reset(options=reset_opts)

  The param threads through: run_episode → run_episodes → evaluate_all.

  The main block now runs BOTH modes:
    summaries_random = evaluate_all(n_episodes=20, fixed_start=False)
    summaries_fixed  = evaluate_all(n_episodes=10, fixed_start=True)
    → prints two comparison tables
    → saves two bar charts (eval_bar_comparison.png + ..._fixed.png)

-----------------------------------------------------------------------
EVALUATION RESULTS
-----------------------------------------------------------------------

MODE 1: RANDOM START (N=20)
  Policy               Lap%    Reward    Speed(m/s)  LatErr(m)  Laps
  -------------------------------------------------------------------
  Expert (rule)       30.0% <-  856.4 <-    8.53      1.494    3.30 <-
  BC (imitation)      25.0%     709.8       7.75      1.495    2.75
  PPO + BC + Stable    0.0%     298.9      15.63 <-   1.001 <- 1.25
  PPO + BC + Curriculum 0.0%   160.0      14.34      1.153    0.80
  -------------------------------------------------------------------
  <- = best in column

  Key observations:
    - PPO rewards are now MUCH higher than d13 (299 vs 62, 160 vs 91).
      The lap bonus explains this: Stable gets ~1.25 laps × 100 = +125.
    - Expert reward jumped from 618 (d13) to 856: 3.30 laps × 100 = +330.
    - The LAP BONUS is doing its job — it's the dominant term for policies
      that actually complete laps.

MODE 2: FIXED START (N=10)
  Policy               Lap%     Reward    Speed(m/s)  LatErr(m)  Laps
  -------------------------------------------------------------------
  Expert (rule)      100.0% <- 2909.1      17.10      0.797   11.00 <-
  BC (imitation)     100.0%    2909.1 <-   17.03      0.756 <- 11.00
  PPO + BC + Stable    0.0%     135.1      24.60 <-   0.874    0.00
  PPO + BC + Curriculum 0.0%   258.9      25.00       0.981    1.00
  -------------------------------------------------------------------
  <- = best in column

  std_reward = 0.00 for all: with a deterministic policy and a fixed start,
  every episode is IDENTICAL. The same state every reset → the same
  deterministic action sequence → the same outcome.

-----------------------------------------------------------------------
RESULT ANALYSIS
-----------------------------------------------------------------------

FIXED-START REVEALS A SURPRISING RESULT:

  Expert and BC score 100% lap completion and 11 laps per episode.
  PPO Stable and Curriculum score 0% (they crash before completing
  a full 2000-step episode), despite going 24–25 m/s.

  What is happening?
    The DynamicCar at 24–25 m/s produces extreme tyre slip.
    The expert was programmed conservatively: it brakes for corners.
    At 17 m/s (fixed start) it never exceeds the lateral error threshold.
    At 25 m/s, the PPO agents generate v_y (lateral sliding) they haven't
    learned to manage over a full lap, and eventually spin out.

  This is NOT the result we expected. We expected PPO to win decisively
  on fixed start. Instead:
    - Faster ≠ better, because the DynamicCar makes speed expensive.
    - Expert/BC with conservative speed are more reliable over 2000 steps.

  What this means for the project:
    The PPO agents need LONGER curriculum training (Stage 3 was still
    improving at 1M steps) to learn tyre management at 25 m/s.
    After ~3M steps we would likely see PPO lap-completion rate improve.

THE LAP BONUS CHANGES THE REWARD LANDSCAPE:
  Before d14: expert scored 618 (random start), curriculum scored 91.
  After  d14: expert scores 856 (random start), curriculum scores 160.
  The lap bonus adds a clear signal for sustained lap completion.
  Future training runs with d14's reward will have a stronger gradient
  toward actually finishing laps, not just going fast and crashing.

RANDOM-START vs FIXED-START SPEEDS:
  Expert:  8.53 m/s (random) vs 17.10 m/s (fixed)
    Random start: some episodes start badly (wrong heading, braking, slow).
    Fixed start: always clean, always fast from the start.
  This shows random-start evaluation underestimates peak speed for all policies.

EVALUATE.PY LAP-COUNTING SIMPLIFICATION:
  Previously run_episode() imported closest_point() locally and ran its
  own wrap-around detection loop. This was a duplicate of the logic now
  in F1Env.step(). After d14, run_episode reads env.laps_completed
  directly — one source of truth. The local import and detection loop
  were removed.

-----------------------------------------------------------------------
KEY ML CONCEPTS LEARNED THIS SESSION
-----------------------------------------------------------------------

1. Discrete event bonuses vs continuous shaping
   Per-step shaped rewards (progress, lateral, heading) form a smooth
   gradient field the agent follows every timestep.
   Discrete event bonuses (lap completion = +100) fire once per event.
   Mixing them is fine but they should be kept computationally separate.
   In our implementation: reward = reward_fn.compute(...) + lap_bonus.
   The event bonus lives in the env, not in the shaping function.

2. Evaluation distribution != training distribution (and why it matters)
   Policies trained with random starts learn to be robust.
   Evaluating with random starts measures robustness — not peak skill.
   For racing, "robustness to bad starts" is less relevant than
   "skill from the start line." Always use domain-appropriate evaluation.
   Report BOTH: random-start (robustness) and fixed-start (skill).

3. Deterministic policy + fixed environment = zero variance
   Fixed start + deterministic=True gives std_reward=0.00.
   This is because there is no source of randomness: same obs every reset,
   same action from the policy, same physics. N=10 episodes give exactly
   the same result each time. In this setting, N=1 would suffice — but
   N=10 is safer if you accidentally have stochastic components.

4. Gym options dict as a clean API for conditional resets
   The Gymnasium spec allows options: dict to be passed to reset().
   Using options={"fixed_start": True} avoids subclassing F1Env or
   adding a boolean attribute to the environment. The env stays clean,
   and callers opt in explicitly.

5. Single source of truth for computed metrics
   Before d14: lap counting existed in BOTH evaluate.py (closest_point
   loop) and as documentation-only in f1_env.py.
   After d14: it lives only in f1_env.step(), exposed via info dict
   and env.laps_completed. evaluate.py is a consumer, not a recomputer.
   This is the DRY (Don't Repeat Yourself) principle applied to metrics.

-----------------------------------------------------------------------
WEEK 4 STATUS
-----------------------------------------------------------------------
COMPLETED:
  <- Lap-aware reward: +100 per completed lap, lives in env/f1_env.py
  <- Fixed-start evaluation: options={"fixed_start": True} in reset()
  <- Both evaluation modes running: random-start and fixed-start tables
  <- Four plots saved (2 bar charts + trajectories + distributions)
  <- evaluate.py lap counting consolidated: one source of truth

WHAT FIXED-START REVEALED:
  PPO policies go 24–25 m/s but crash before completing sustained laps.
  This points clearly to the next step:

REMAINING WEEK 4:
  -> Longer curriculum training (3M+ steps): Stage 3 reward was still
     climbing at 1M steps (~1100). More training would likely fix the
     25 m/s instability. Expected outcome: PPO wins fixed-start eval.
  -> Multi-lap training: modify env to not reset on truncation, only on
     crash. Let the agent run indefinitely across laps.
  -> Load transfer under braking: add longitudinal weight shift to
     DynamicCar (front grip increases under braking, rear decreases).
  -> Tyre degradation: grip = 1.5 - 0.0002 * step_count. Forces agent
     to manage tyre life, introducing real F1 race strategy decisions.
