DAY 17 -- Tyre Degradation: Implementation + Obs Extension
===========================================================
Week 5 | F1 race strategy begins — grip decays, pace must be managed

-----------------------------------------------------------------------
SESSION GOAL
-----------------------------------------------------------------------
Add realistic tyre degradation to DynamicCar so that the agent must
learn to manage grip across a stint, not just maximise instantaneous
speed.  This is the central problem of real F1 race strategy:
  - Push too hard → tyres die early → episode ends early → less reward
  - Go too easy   → survive longer  → but lower per-step reward
The agent must find the pace that maximises TOTAL episode reward.

This is also the foundation for Week 5's next step: pit stops.
Once the agent knows its tyres are degrading, it will need to decide
WHEN to sacrifice lap time for fresh rubber.

-----------------------------------------------------------------------
WHY TYRE DEGRADATION?
-----------------------------------------------------------------------
All training up to d16 used mu = 1.5 forever.  Every lap was identical.
The agent learned: "go as fast as possible."  That's not wrong — it just
doesn't capture the real problem.

In an F1 race, tyre degradation forces a strategic trade-off:
  - Lap 1-5:   Tyres are fresh, maximum grip, can push hard.
  - Lap 8-12:  Tyres are worn, mu drops, cornering becomes harder.
  - Lap 14+:   Tyres are critically worn, the car slides continuously
                and is likely to go off-track.

The driver (or RL agent) must:
  1. Know how worn the tyres are (from the tyre_life observation).
  2. Modulate pace — more throttle/slip now means less grip later.
  3. Eventually pit (d18) or manage to the end of the stint.

This creates EMERGENT STRATEGY: the agent discovers conservative pacing
on its own, from the reward signal alone.  We do not tell it "drive
slower in corners when tyre_life < 0.5."  It discovers this.

-----------------------------------------------------------------------
FILES MODIFIED / CREATED
-----------------------------------------------------------------------
env/car_model.py       -- DynamicCar: tyre degradation physics.
                          New params: base_wear=0.0003, slip_coef=0.002
                          New attribute: tyre_life (starts 1.0)
                          New attribute: mu_base (stores original mu)
                          DynamicCar.step(): degrades mu after slip angles
                          DynamicCar.reset_tyres(): restores tyre_life=1.0

env/f1_env.py          -- Added tyre_degradation=False param to __init__().
                          When True:
                            - obs is 12D (appends tyre_life ∈ [0, 1])
                            - observation_space is Box(12,)
                            - reset() calls car.reset_tyres()
                            - info includes "tyre_life" key
                          When False: all existing behavior unchanged.

rl/make_env.py         -- Added make_env_tyre() factory.
                          Returns F1Env(tyre_degradation=True) in Monitor.

rl/bc_init_policy.py   -- Added extend_obs_dim(model, old_dim, new_dim).
                          Zero-pads first layer of both actor and critic
                          from 11D to 12D.  Policy behavior is IDENTICAL
                          to v2 on step 0 (new dim has zero weight).

rl/train_ppo_tyre.py   -- New training script.
                          Loads ppo_curriculum_v2.zip, extends 11→12D,
                          runs 2M steps in tyre env, saves ppo_tyre.zip.

-----------------------------------------------------------------------
TYRE DEGRADATION MODEL
-----------------------------------------------------------------------

Physics (in DynamicCar.step(), after slip angle computation):

  wear = base_wear + slip_coef * (|alpha_f| + |alpha_r|)
  tyre_life = max(0.0, tyre_life - wear)
  mu = mu_base * max(0.1, tyre_life)

Parameters:
  base_wear = 0.0003 per step   — rolling attrition (even in straight line)
  slip_coef = 0.002 per rad     — extra wear from tyre sliding
  mu_base   = 1.5               — original peak friction coefficient
  mu floor  = mu_base * 0.1 = 0.15  — minimum grip (car still controllable)

WHY SLIP-BASED WEAR?
  Constant degradation would make all pace strategies equally costly.
  Slip-based wear creates the real F1 physics:
    - Going straight, barely sliding: near-zero extra wear
    - Cornering with high slip angle: significant wear
    - Aggressive oversteer: extreme wear

  This means the AGENT'S CHOICES directly determine tyre life.
  Conservative cornering → less slip → slower wear → longer stint.
  Aggressive cornering  → more slip → faster wear → shorter stint.

HOW QUICKLY TYRES WEAR:

  Driving style     alpha_f+alpha_r  wear/step  worn at step  laps
  ----------------------------------------------------------------
  Very conservative   0.05 rad       0.000400     2500 steps   ~21
  Normal (v2 style)   0.20 rad       0.000700     1429 steps   ~12
  Aggressive          0.50 rad       0.001300      769 steps    ~6
  ----------------------------------------------------------------
  Episode cap: 2000 steps (200 seconds).  Normal driving wears out
  BEFORE the episode ends — the physics forces a trade-off.

WHAT MU DOES:
  mu enters the Pacejka formula as the peak lateral force coefficient:
    D = mu * F_z   (peak lateral force)
  Lower mu → lower peak lateral force → less cornering grip.
  At tyre_life = 0.5: mu = 0.75.  At 0.3: mu = 0.45.
  Below ~0.3 tyre_life, the car can no longer hold corners at full speed.

-----------------------------------------------------------------------
OBSERVATION EXTENSION: 11D → 12D
-----------------------------------------------------------------------

Old obs (11D, d1-d16):
  [0]  v / 20.0              speed
  [1]  heading_error / π     track alignment
  [2]  lateral_error / 3.0   distance from centreline
  [3]  sin(heading_error)     smooth angle encoding
  [4]  cos(heading_error)     smooth angle encoding
  [5]  curv_near / π          curvature 5 wpts ahead
  [6]  curv_mid  / π          curvature 15 wpts ahead
  [7]  curv_far  / π          curvature 30 wpts ahead
  [8]  progress               lap position [0, 1]
  [9]  v_y / 5.0              lateral sliding velocity
  [10] r   / 2.0              yaw rate

New obs (12D, tyre env, d17+):
  [0-10] same as above
  [11] tyre_life ∈ [0, 1]    1.0 = new tyres, 0.0 = fully worn

WHY INCLUDE TYRE_LIFE IN OBS?
  Without it, the agent is blind to its tyre state.  It cannot develop
  strategy because it cannot distinguish:
    "I have 80% tyre left"  from  "I have 20% tyre left"

  With tyre_life in obs, the network CAN learn conditional behavior:
    if tyre_life > 0.7: push hard (safe, plenty of grip)
    if tyre_life < 0.3: back off (grip nearly gone, crash imminent)

  This information-theoretic change is necessary for any emergent
  strategy to appear.

-----------------------------------------------------------------------
WEIGHT EXTENSION: HOW 11D → 12D IS HANDLED
-----------------------------------------------------------------------

Problem:
  ppo_curriculum_v2 has actor/critic networks with input dim = 11.
  The tyre env has obs dim = 12.  Direct loading fails (shape mismatch).

Solution (extend_obs_dim in bc_init_policy.py):
  1. Load v2 WITHOUT env (no shape check).
  2. For actor trunk (mlp_extractor.policy_net[0]) and critic trunk
     (mlp_extractor.value_net[0]):
       old_weight: shape (128, 11)
       new_weight: shape (128, 12), columns [:, :11] copied, [:, 11] = 0
  3. Update in_features = 12 on both layers.
  4. Update model.observation_space to Box(12,).
  5. Call model.set_env(tyre_env) — now shapes match.

Key property: zero-initialization of the new column.
  The new tyre_life dimension has zero weight in both actor and critic
  at the start of training.  This means:
    - Policy's outputs at step 0 are IDENTICAL to v2's outputs.
    - No sudden gradient spike from the new dimension.
    - The network gradually LEARNS to use tyre_life through backprop.

  This is the same technique used to extend vocabulary embeddings in
  language models: add new rows/columns initialized to zero.  The model
  retains all old knowledge; new knowledge is learned incrementally.

Compare to d16 (catastrophic forgetting):
  d16 failed because EPISODE TERMINATION changed (2000 → infinite).
  The value function's calibration was completely wrong.
  This run keeps termination UNCHANGED (still max_steps=2000).
  Only the obs dim and tyre physics change — the value function's
  horizon is still correct.  Risk of forgetting is minimal.

-----------------------------------------------------------------------
TRAINING DESIGN
-----------------------------------------------------------------------

Checkpoint:         ppo_curriculum_v2.zip (3M steps, 27 m/s, 100% lap%)
Additional steps:   2M steps (total ~5M)
Obs dim:            12D (11D + tyre_life)
Tyre wear:          base=0.0003/step + 0.002×slip/step
Episode length:     max_steps=2000 (unchanged from all previous training)
LR:                 1e-4 → 1e-6 (cosine decay, same as d15)

WHY 1e-4 (HIGHER THAN D16's 5e-5)?
  d16 tried 5e-5 and still catastrophically forgot — but the root cause
  was value distribution shift, NOT a high LR.  A lower LR would not
  have saved d16.

  Here, value distribution shift is not a risk (termination unchanged).
  We use 1e-4 because we WANT the network to learn the tyre_life
  dimension quickly.  Too low an LR would leave the new column at near-
  zero for the entire 2M steps — the agent would never develop strategy.

EXPECTED BEHAVIOR (pre-training):
  Early training (0-500k steps):
    The tyre_life weight is near zero.  Policy looks like v2.
    Episodes end earlier than v2 (tyres die) → ep_len_mean drops.
    ep_rew_mean will drop from v2's 2250 as episodes get shorter.

  Mid training (500k-1.5M):
    The network begins to read tyre_life.  Policy gets slightly more
    conservative in corners.  Episodes get longer again.
    ep_rew_mean recovers toward v2 levels.

  Late training (1.5M-2M):
    Policy has learned to manage tyre degradation.  Episodes may
    run longer than v2's 1010 steps if the agent drives conservatively
    enough to keep tyres alive until episode truncation.
    ep_rew_mean target: > 1500 (vs v2's 2250 with perfect tyres).

-----------------------------------------------------------------------
TRAINING RESULTS (WHAT ACTUALLY HAPPENED)
-----------------------------------------------------------------------

Bug encountered and fixed:
  PPO.load() creates the rollout buffer using the LOADED obs space (11D).
  extend_obs_dim() updates the policy network and model.observation_space,
  but the rollout buffer retains shape (11,).  On the first collect_rollouts()
  call, SB3 tries to store 12D observations into an 11D buffer →
  ValueError: shape (1,12) into (1,11).

  Fix: after set_env(), explicitly recreate RolloutBuffer with the new
  12D observation_space.  All other hyperparams stay the same.
  Lesson: when extending obs dim mid-training, ALSO recreate the buffer.

Training arc (ep_len_mean / ep_rew_mean):
  Start  (~3.0M steps):  126 steps /   72 reward  (tyres dying immediately)
  Peak   (~4.9M steps):  607 steps /  642 reward  (agent learned tyre management)
  End    (~5.0M steps):  543 steps /  508 reward  (slight regression at end)

What happened mechanically:
  At the start, the tyre_life dimension had zero weight.  The agent drove
  exactly like v2 (27 m/s, high slip angles in corners).  With base_wear
  + slip_wear ≈ 0.0013/step, tyres died in ~126 steps.

  Over 2M steps, the network learned to use obs[11].  It discovered that
  reducing speed → less slip → less wear → longer episodes → more laps.
  ep_len_mean climbed from 126 → 607 (4.8× improvement).

  The slight regression at the end (607 → 543) likely reflects the LR
  bottoming out at 1e-6 with the policy oscillating near its optimum.

-----------------------------------------------------------------------
EVALUATION RESULTS (ppo_tyre vs all previous policies)
-----------------------------------------------------------------------

RANDOM START (N=20):
  Policy                    Lap%   Reward  Speed(m/s)  LatErr(m)  Laps
  -------------------------------------------------------------------
  Expert (rule)             35%    1002.4      8.89      1.386    3.85
  BC (imitation)            50% <- 1442.2     10.92      1.217    5.50
  PPO + BC + Stable          0%     114.9     13.76      1.268    0.55
  PPO + BC + Curriculum      0%     149.4     14.14      1.309    0.75
  PPO v2 (3M)               45%    2012.8 <- 15.29 <-   1.210 <- 7.65 <-
  PPO Multi-Lap (5M)         0%      -5.9      9.43      1.240    0.10
  PPO Tyre (5M)              0%     578.1      9.55      1.699    2.95
  -------------------------------------------------------------------

FIXED START (N=10):
  Policy                    Lap%   Reward  Speed(m/s)  LatErr(m)  Laps  Steps
  -------------------------------------------------------------------------
  Expert (rule)            100% <- 2909.1     17.10      0.797    11.0   2000
  BC (imitation)           100%    2909.1     17.03      0.756    11.0   2000
  PPO + BC + Stable          0%     135.1     24.60      0.874     0.0    122
  PPO + BC + Curriculum      0%     258.9     25.00      0.981     1.0    140
  PPO v2 (3M)              100%    4531.7 <- 26.92 <-   0.678 <- 17.0   2000
  PPO Multi-Lap (5M)         0%     -10.2     14.49      1.256     0.0     15
  PPO Tyre (5M)              0%    1643.5     16.09      2.375     8.0   1531
  -------------------------------------------------------------------------

MOST IMPORTANT NUMBER: Fixed start, steps = 1531.
  The tyre agent survived 1531 steps vs the starting point of 126 steps.
  That is a 12× improvement in stint length.  It completed 8 laps before
  the tyres ran out at step 1531 and the car crashed.

-----------------------------------------------------------------------
ANALYSIS: DID EMERGENT TYRE MANAGEMENT APPEAR?
-----------------------------------------------------------------------

YES. Two clear signals:

1. Speed reduction: v2 ran at 26.92 m/s (fixed start).
   ppo_tyre ran at 16.09 m/s — a 40% speed reduction.
   The agent was NOT told to go slower.  It discovered that going slower
   = less slip = less tyre wear = longer survival = more lap bonuses.
   This is the emergent strategy we were looking for.

2. Stint extension: 126 steps (start) → 1531 steps (end of training).
   The agent extended its own life by 12× by learning to conserve tyres.
   With pure base_wear at 1531 steps: 1531 × 0.0003 = 0.46 wear.
   Remaining 0.54 wear came from cornering slip over 1531 steps =
   0.00035/step → alpha ≈ 0.18 rad.  That is genuinely conservative.
   Compare: v2-style driving gave alpha ≈ 0.5+ rad (worn in 126 steps).

WHY DID LAP COMPLETION STAY AT 0%?
  The tyre agent always crashes before step 2000 (tyres run out).
  Since "lap completion" = survived full 2000 steps, 0% is correct.
  This is the fundamental limitation of the no-pit-stop setup:
  without pit stops (d18), the agent has no way to reset tyres.
  It manages its single set of tyres as long as possible, then crashes.

WHY IS LATERAL ERROR SO HIGH (2.375 m vs v2's 0.678 m)?
  Two contributing factors:
  a) By step 1200-1531, tyre_life is low → mu is reduced → car slides
     more even at the same steering inputs → lateral error increases.
  b) The agent sacrificed some lateral precision for speed reduction.
     Going slower reduced slip-wear but some of the driving polish
     from v2 may have been lost during the 2M fine-tuning steps.

HOW IT COMPARES TO MULTI-LAP (d16):
  d16 was a catastrophic failure — policy became worse than BC.
  d17 maintained meaningful capability: 8 laps, 16 m/s, 1531 steps.
  The key difference: this run kept max_steps=2000 (no termination change)
  and used obs extension with zero-init (no forgetting risk).
  Result: safe fine-tuning with genuine emergent behaviour.

-----------------------------------------------------------------------
BUGS FOUND AND FIXED (d17)
-----------------------------------------------------------------------

1. Rollout buffer shape mismatch after obs dim extension:
   When loading a model with PPO.load() and then extending the obs dim,
   the internal rollout buffer still has the old shape.  SB3 tries to
   store 12D observations into an 11D buffer → ValueError.
   Fix: recreate RolloutBuffer after set_env() with the new obs space.
   This fix is documented in the training script and in bc_init_policy.py.

2. DynamicCar default degradation was non-zero:
   Initial implementation had base_wear=0.0003, slip_coef=0.002 as
   DEFAULTS on DynamicCar.  This broke all existing training and evaluation
   (every env would silently degrade grip, even non-tyre envs).
   Fix: defaults set to 0.0.  F1Env(tyre_degradation=True) passes the
   actual values when constructing DynamicCar.
   Caught by unit test: "F1Env default: tyre_life must be 1.0 in info."

-----------------------------------------------------------------------
WHAT'S NEXT (d18)
-----------------------------------------------------------------------

The natural next step is pit stops.  Right now the agent has no escape
from worn tyres — it just degrades until it crashes.  Adding a pit action:

  Action space: Box(3,)  — [throttle, steer, pit_signal]
  pit_signal > 0 → pit this step
  Pit effect: car.reset_tyres() + reward penalty (time cost, e.g. -200)

  This requires a new BC policy (3D actions) and retraining from scratch
  since the action space changes.

  With pits, the agent must learn WHEN to pit:
    Too early: waste fresh tyre advantage at the start of the stint
    Too late:  worn tyres reduce lap time, may crash before pitting
    Optimal:   balance tyre management vs pit stop time penalty

  This is the full F1 strategy problem.  Emergent pit timing would be
  the centrepiece result of Week 5.

-----------------------------------------------------------------------
KEY ML CONCEPTS THIS SESSION
-----------------------------------------------------------------------

1. Input dimension extension via zero-padding
   When a pre-trained network's input size grows, the principled
   approach is to copy old weights and zero-initialise new columns.
   The policy starts identical to the pre-trained version; new weights
   grow from zero via backprop.  No forgetting of old knowledge.

   Alternative: re-train from scratch (wasteful — 3M steps of driving
   knowledge thrown away).

2. Environment change classification
   Not all env changes are equally dangerous for continued training.
   Two categories:
     SAFE:   Adding new obs dimensions (zero-padded), changing physics
             parameters (drag, tyre slip).  Value function horizon
             unchanged.  Low forgetting risk.
     UNSAFE: Changing episode termination (truncated/terminated logic).
             Value function calibration is immediately wrong.
             High forgetting risk (d16).

   Rule: if you must change termination, re-calibrate the critic first
   (critic-only warm-up, or graduated changes).

3. Emergent strategy via reward shaping through physics
   We do NOT add an explicit tyre_life reward term.  The agent learns
   to conserve tyres purely because:
     - Low tyre_life → high slip → crashes → negative reward
     - The agent observes tyre_life → can predict when to slow down
     - Conserving tyres → longer episodes → more lap bonuses
   The physics creates the incentive structure; the agent discovers it.

   This is the cleaner approach vs hardcoded reward engineering.
   The agent may discover strategies we didn't anticipate.

-----------------------------------------------------------------------
WHAT'S NEXT
-----------------------------------------------------------------------
After evaluating ppo_tyre, the natural next step is d18: pit stops.

The agent currently has no way to escape worn tyres — it just degrades
until it crashes.  Adding a pit action means:
  - Agent gains 3rd action dimension: pit_signal ∈ [-1, 1]
  - If pit_signal > 0: tyre reset (car.reset_tyres()), minus time penalty
  - Action space: Box(3,) — requires new BC + new training from scratch
  - The strategic question becomes WHEN to pit, not just how hard to push

Alternatively: accept d17 as the tyre management result and move to
load transfer under braking (more realistic vehicle dynamics).
