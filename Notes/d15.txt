DAY 15 -- Continued Training: PPO Curriculum v2 (3M steps)
==========================================================
Week 4 (continued) | Fixing 0% fixed-start lap completion for PPO

-----------------------------------------------------------------------
SESSION GOAL
-----------------------------------------------------------------------
d14's fixed-start evaluation exposed a clear problem:
  PPO Curriculum v1 (1M steps) went 25 m/s but showed 0% lap completion
  even from a clean fixed start. The agent was too aggressive — it
  hadn't learned to manage tyre slip over a full 2000-step episode.

  The fix: continue Stage 3 training from the existing checkpoint for
  2M more steps, using the new lap-bonus reward as the training signal.

-----------------------------------------------------------------------
NEW FILE CREATED
-----------------------------------------------------------------------
rl/train_ppo_continue.py   -- loads ppo_curriculum.zip, runs 2M more
                              Stage 3 steps, saves ppo_curriculum_v2.zip

FILES MODIFIED:
  rl/evaluate.py            -- added PPO Curriculum v2 as an optional
                               policy entry; fixed optional_file key
                               so each policy specifies its own filename
                               (was hardcoded to ppo_curriculum.zip).

New artifacts:
  rl/ppo_curriculum_v2.zip
  runs/ppo_curriculum_v2/
  plots/eval_bar_comparison.png          (updated with 5th policy)
  plots/eval_bar_comparison_fixed.png    (updated with 5th policy)
  plots/eval_trajectories.png            (updated)
  plots/eval_reward_distribution.png     (updated)

-----------------------------------------------------------------------
TRAINING DESIGN
-----------------------------------------------------------------------

WHY CONTINUE FROM CHECKPOINT vs RETRAIN FROM SCRATCH:
  Retraining from scratch would redo Stages 1 + 2 (~100k steps)
  before arriving at Stage 3 — wasted compute on already-solved problems.
  The policy already knows how to race at ~15 m/s.  Continuing Stage 3
  spends ALL 2M new steps on the actual problem: sustained laps at 25 m/s.

WHY THE LAP BONUS DOESN'T BREAK CONTINUITY:
  PPO is on-policy: every rollout generates fresh transitions from the
  CURRENT policy.  Old rollouts from ppo_curriculum.zip are discarded.
  The agent immediately receives +100 signals for crossing the start line,
  giving it a clear gradient toward staying alive to lap completion.

HYPERPARAMETER CHANGE:
  Original curriculum LR:  3e-4 → 1e-6 (cosine, 1M steps)
  Continued run LR:         1e-4 → 1e-6 (cosine, 2M steps)

  WHY lower starting LR for continued training?
    The policy is already well-trained.  Large gradient steps could
    overwrite good behaviours.  Starting at 1e-4 (1/3 of original)
    gives smaller, more targeted updates — this is "fine-tuning mode."
    Analogy: an F1 driver in the last stint doesn't reinvent their
    braking points, they refine them.

SB3 MECHANICS:
  model = PPO.load(checkpoint_path, env=env, device=device)
  model.learn(total_timesteps=2_000_000, reset_num_timesteps=False)

  reset_num_timesteps=False preserves the global step counter (starts
  at ~1M, ends at ~3M). TensorBoard shows a continuous curve.
  The optimizer state is also preserved — Adam's momentum estimates
  carry over, giving warmer gradient updates from step 1.

TRAINING METRICS (at end of run, ~3M total steps):
  ep_rew_mean:  ~2,250     (up from ~1,100 at end of v1)
  ep_len_mean:  ~1,010     (agent surviving ~half the episode on average)
  learning_rate: 1e-6      (cosine schedule reached minimum)
  total_timesteps: 3,002,368

-----------------------------------------------------------------------
EVALUATION RESULTS
-----------------------------------------------------------------------

RANDOM START (N=20):
  Policy               Lap%    Reward    Speed(m/s)  LatErr(m)  Laps
  -------------------------------------------------------------------
  Expert (rule)        35%   1002.7        9.09      1.350    3.85
  BC (imitation)       25%    714.5        7.50      1.443    2.80
  PPO + BC + Stable     0%    220.7       15.24      1.044 <- 1.05
  PPO + BC + Curriculum 0%    130.6       15.45 <-   1.230    0.55
  PPO Curriculum v2    40% <- 1775.8 <-   14.73      1.256    6.75 <-
  -------------------------------------------------------------------

FIXED START (N=10, deterministic — std=0):
  Policy               Lap%     Reward    Speed(m/s)  LatErr(m)  Laps
  -------------------------------------------------------------------
  Expert (rule)       100% <-  2909.1       17.10      0.797   11.00
  BC (imitation)      100%     2909.1       17.03      0.756 <- 11.00
  PPO + BC + Stable     0%      135.1       24.60      0.874    0.00
  PPO + BC + Curriculum 0%      258.9       25.00      0.981    1.00
  PPO Curriculum v2   100% <-  4531.7 <-   26.92 <-   0.678 <- 17.00 <-
  -------------------------------------------------------------------
  <- = best in column

-----------------------------------------------------------------------
RESULT ANALYSIS
-----------------------------------------------------------------------

THE RANKING FLIPPED ON FIXED START:
  v1 (1M steps): 0% lap completion, 25 m/s, crashes
  v2 (3M steps): 100% lap completion, 26.9 m/s, 17 laps, 4532 reward

  This is the result we predicted in d14: the additional 2M steps of
  Stage 3 training taught the agent to manage tyre slip at sustained
  speed.  It now WINS on every fixed-start metric simultaneously —
  fastest speed, lowest lateral error, most laps, highest reward.

  The lap bonus was essential to this flip.  Without +100 per lap,
  the agent had no reason to stay alive past the first lap crossing.
  With it, surviving to cross the line multiple times is the dominant
  gradient.

V2 SPEED vs EXPERT:
  v2: 26.92 m/s   Expert: 17.10 m/s   → +57% faster
  v2: 17 laps     Expert: 11 laps     → +55% more laps per episode
  v2: 4532 reward Expert: 2909 reward → +56% higher reward

  The expert was designed to be conservative.  v2 found a more
  aggressive strategy that the programmer never encoded — exactly
  the promise of RL over rule-based systems.

RANDOM-START RESULT: v2 WINS TOO
  On random start, v2 also wins: 40% lap completion, 1776 reward,
  6.75 laps.  The expert had 35% / 1003 / 3.85 laps.
  v2 is now the best policy on BOTH evaluation modes.

WHY V2 WINS RANDOM-START BUT V1 DIDN'T:
  v1 went 25 m/s and crashed on bad starts (0% lap completion).
  v2 goes 14.7 m/s on average during random-start eval — SLOWER than v1.
  This suggests v2 learned speed management: it can modulate between
  fast stretches and slower corners, instead of holding max throttle.
  That modulation is what survives bad starts AND completes more laps.

THE REWARD NUMBERS:
  Original v1 random-start reward: ~91 (d13, no lap bonus)
  v2 random-start reward: 1,776 (includes lap bonus: 6.75 laps × 100 = 675)
  v2 base reward (ex-bonus): ~1,101 — still roughly 12x higher than v1
  This shows the policy genuinely improved, not just from the bonus.

EP_REW_MEAN DURING TRAINING:
  v1 end (~1M steps): ~1,100
  v2 mid (~2.3M):     ~2,270  (with lap bonus)
  v2 end (~3M):       ~2,250  (plateau — reward stopped climbing)

  The plateau at ~2,250 suggests the policy is near its capability
  limit for this reward function and episode length.  If we want
  further improvement the next lever is EITHER:
    a. Longer episodes (multi-lap training — reset only on crash)
    b. A harder reward (tyre degradation, fuel strategy)

-----------------------------------------------------------------------
KEY ML CONCEPTS LEARNED THIS SESSION
-----------------------------------------------------------------------

1. Checkpoint continuation in on-policy RL
   PPO is on-policy: only uses rollouts from the current policy.
   Continuing from a checkpoint is therefore always equivalent to
   "training from this starting point" — there is no off-policy
   correction needed.
   With SB3: PPO.load() + model.learn(reset_num_timesteps=False).
   The optimizer state is preserved, giving warmer gradient momentum
   from step 1 of the continued run.

2. Fine-tuning LR schedule
   When continuing from a well-trained checkpoint, reduce the initial LR.
   3e-4 is sized for random initialization (large steps needed to escape
   bad regions). 1e-4 is appropriate for fine-tuning (small refinements).
   General rule: LR should scale with how far the optimal policy is
   from the current policy. Close = small LR.

3. Lap bonus as a curriculum-within-Stage-3
   The lap bonus didn't change the physics or the action space.
   It added one new gradient direction: complete laps.
   This is analogous to a Stage 4 without a formal curriculum callback:
   the policy was nudged from "go fast" to "go fast AND finish laps"
   purely through reward shaping.

4. Speed management as emergent behaviour
   v2's random-start speed (14.7 m/s) is LOWER than v1 (15.4 m/s),
   yet v2 completes far more laps and scores much higher reward.
   The agent learned that lower average speed + more lap completions
   outscores higher average speed + crashes.
   This is speed management: a behaviour that wasn't explicitly
   programmed, just incentivised.

5. Plateau detection
   ep_rew_mean stopped climbing at ~2,250 after ~2.5M steps.
   This is a reliable signal that the policy has saturated its
   current reward function. The next improvement requires changing
   either the environment or the reward, not just adding steps.
   Recognising this plateau avoids wasting compute.

-----------------------------------------------------------------------
FULL PROJECT PERFORMANCE HISTORY (fixed start, all models)
-----------------------------------------------------------------------

  Model                      Steps   Lap%   Speed   Laps   Reward
  ----------------------------------------------------------------
  Expert (rule-based)          --    100%   17.1    11.0    2909
  BC (imitation)               --    100%   17.0    11.0    2909
  PPO from scratch (kinematic)  300k    --     2.9    --      --
  PPO + BC + Stable (kinematic) 300k    --    19.4    --      --
  PPO + BC + Stable (dynamic)   300k     0%   24.6     0     135
  PPO + BC + Curriculum v1      1M       0%   25.0     1     259
  PPO + BC + Curriculum v2      3M     100%   26.9    17    4532  <-- BEST
  ----------------------------------------------------------------

The complete arc of the project:
  d1-d8:   environment + expert + BC (imitation matched expert)
  d9-d11:  PPO + BC warm start (2x faster than expert, kinematic model)
  d12:     DynamicCar (harder physics, PPO needed to relearn)
  d13:     Curriculum (3 stages, speed improved over stable baseline)
  d14:     Lap bonus + fixed-start eval (exposed 0% lap completion bug)
  d15:     3M step continued training (fixed it: 100% @ 26.9 m/s)

-----------------------------------------------------------------------
WEEK 4 STATUS
-----------------------------------------------------------------------
COMPLETED:
  <- Lap-aware reward (+100 per lap) — d14
  <- Fixed-start evaluation (removes random-start bias) — d14
  <- Continued training to 3M steps — d15
  <- PPO v2 now wins on ALL metrics vs expert and all previous policies

REMAINING WEEK 4 OPTIONS:
  -> Multi-lap training: reset only on crash (not on max_steps),
     let the agent run indefinitely across laps.
     With the plateau at ep_len ~1,000, removing the 2000-step limit
     would let the agent discover longer strategies.

  -> Load transfer under braking: add longitudinal weight shift to
     DynamicCar (front grip increases under braking).

  -> Tyre degradation: grip = 1.5 - 0.0002 * step_count.
     Forces the agent to manage tyre life — the core of real F1 race
     strategy. This is the natural Week 5 project.
