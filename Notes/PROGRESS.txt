==============================================================================
F1 AUTONOMOUS RACING AGENT -- FULL PROJECT SUMMARY
==============================================================================
A multi-week project building a racing AI from scratch:
  Expert rules -> Behavioral Cloning -> PPO RL -> Dynamic Physics -> Curriculum

Read the individual day files (d10.txt -- d13.txt) for full technical detail.
This file is the high-level story: what we built, why, and what we proved.

==============================================================================
THE CORE IDEA
==============================================================================

Problem:
  Train an agent to drive a car around a racing circuit as fast as possible
  without going off-track.

Approach (standard robotics pipeline):
  1. Build a physics simulator                     (environment)
  2. Write a rule-based expert driver              (baseline)
  3. Train a neural net to imitate the expert      (Behavioral Cloning)
  4. Use that net as a starting point for RL       (warm start)
  5. Run PPO to improve beyond the expert          (RL fine-tuning)
  6. Add realistic physics                         (dynamic bicycle model)
  7. Add curriculum to handle the harder physics   (curriculum learning)

Why this order matters:
  PPO from scratch on a hard task (fast car, realistic physics) rarely works.
  The agent explores randomly, crashes constantly, and learns nothing.
  BC gives it a working policy on day 1. RL then pushes it beyond human design.
  Curriculum makes the hard physics learnable by starting with the easy version.

==============================================================================
WEEK 1 -- ENVIRONMENT + EXPERT + BEHAVIORAL CLONING  (d1 - d8)
==============================================================================

What was built:
  env/car_model.py        Kinematic bicycle model (4 states: x, y, yaw, v)
  env/f1_env.py           Gymnasium environment wrapping car + track + reward
  env/track.py            Oval track with waypoints, tangent, lateral error utils
  expert/expert_driver.py Rule-based driver: steer toward lookahead point,
                          brake for corners proportional to heading error
  bc/train_bc.py          BCPolicy MLP (6D obs -> [128, ReLU, 128, ReLU] -> 2D action)
                          Trained with MSE loss on expert demonstrations
  bc/expert_data.npz      50 episodes of expert driving, ~30k state-action pairs

Key results:
  Expert driver:    10 m/s average, 5.35 laps per episode
  BC (imitation):   9.8 m/s,        5.40 laps per episode
  BC successfully matched the expert -- imitation learning works.

Key concept: Distribution shift
  BC sees expert states during training. At test time, small errors compound
  into states the expert never visited -> the policy degrades. This is the
  fundamental limitation of pure imitation learning.

==============================================================================
WEEK 2 -- PPO RL PIPELINE  (d9 - d11)
==============================================================================

What was built:
  rl/rewards.py             Modular RacingReward class (5 components):
                              progress = v * cos(heading_error)     weight 1.0
                              speed bonus                           weight 0.1
                              lateral penalty (quadratic)           weight 0.5
                              heading penalty (quadratic)           weight 0.1
                              smoothness penalty (action rate)      weight 0.05
                              terminal penalty (off-track)          20.0
  rl/bc_init_policy.py      Utility: copy BC weights into SB3 PPO actor
  rl/train_ppo_bc_init.py   PPO training with 3 stability improvements:
                              ent_coef=0.005    (entropy regularisation)
                              clip_range=0.1    (tighter than default 0.2)
                              cosine LR decay   (3e-4 -> 1e-6 over training)
  rl/schedules.py           Linear, cosine, warmup+cosine LR schedules
  rl/evaluate.py            Full evaluation pipeline: 4 policies, 5 metrics,
                              3 plot types (bar chart, trajectories, box plots)

Bugs found and fixed:
  1. BC weight transfer was incomplete: only 2 of 3 layers copied.
     Action head (net[4]) was random -> RL had to re-learn the output layer.
     Fix: copy all 3 Linear layers (trunk[0], trunk[2], action_net).

  2. Termination threshold was wrong: threshold 3.0 on the NORMALISED
     lateral error (= 9m raw) almost never fired.
     Fix: threshold 1.0 normalised = 3.0m raw.

  3. bc_init_policy.py had forward() defined INSIDE __init__ as a local
     function, not a class method. Python silently used the parent's forward.
     Fix: rewrote as a clean utility module (no custom policy subclass).

  4. rewards.py corrupted to 14 lines (empty __init__).
     Fix: full rewrite from scratch.

Key results (kinematic Car, 6D obs, 300k steps):
  PPO from scratch:        -82.8 reward,  2.9 m/s,  0.05 laps
  PPO + BC + Stable:     1807.7 reward, 19.4 m/s, 10.75 laps
  Expert (rule-based):    785.9 reward, 10.0 m/s,  5.35 laps
  BC (imitation):         727.6 reward,  9.8 m/s,  5.40 laps

Key result: PPO surpassed the expert it was initialised from.
  19.4 m/s vs 10.0 m/s. 10.75 laps vs 5.35 laps.
  This is the core promise of BC + RL: start at expert level, exceed it.

Key concept: Cold start problem
  PPO from scratch: 0.05 laps, reward -82. It learned to go slowly to
  avoid crashing -- a local optimum. BC warm start escaped this completely.

Key concept: Entropy collapse (the reward dip)
  v2 training showed a reward dip at 150k-250k steps. The policy got
  too narrow (low entropy) -> one bad gradient step caused large relative
  change -> policy "forgot" good behaviours.
  Fix: ent_coef=0.005 penalises low entropy, keeps the distribution wide.

==============================================================================
WEEK 3 -- DYNAMIC PHYSICS + OBSERVATION ENGINEERING + CURRICULUM  (d12 - d13)
==============================================================================

PART B: Observation Engineering  (d12)
---------------------------------------
Expanded observation vector 6D -> 11D:

  [0]  v / 20.0             speed
  [1]  heading_error / pi   angle to track
  [2]  lateral_error / 3.0  distance from centreline
  [3]  sin(heading_error)   smooth angle encoding
  [4]  cos(heading_error)   smooth angle encoding
  [5]  curv_near / pi       curvature  5 wpts ahead  ~0.5 s    (was only this)
  [6]  curv_mid  / pi       curvature 15 wpts ahead  ~1.5 s    NEW
  [7]  curv_far  / pi       curvature 30 wpts ahead  ~3.0 s    NEW
  [8]  progress             lap position in [0, 1]              NEW
  [9]  v_y / 5.0            lateral sliding velocity            NEW (Part A)
  [10] r   / 2.0            yaw rate                            NEW (Part A)

Why multi-step curvature: one curvature sample tells you what to do right now.
At 19 m/s, 5 waypoints is 0.5 s. Not enough to plan a braking point.
Three samples give the agent a radar horizon: immediate, near, far.

PART A: Dynamic Bicycle Model  (d12)
--------------------------------------
Replaced kinematic Car with DynamicCar:

  Old Car (kinematic):  4 states [x, y, yaw, v], no tyre slip
  New DynamicCar:       6 states [x, y, psi, v_x, v_y, r]

  New physics:
    Tyre slip angles:
      alpha_f = delta - arctan((v_y + a*r) / v_x)   front
      alpha_r =         -arctan((v_y - b*r) / v_x)  rear

    Pacejka Magic Formula (tyre force):
      F_y = D * sin(C * arctan(B * alpha))
      D = mu * F_z    (peak force, scales with tyre load)
      C = 1.3         (shape factor)
      B = C_tyre / (C * D)  (stiffness factor)
      At small alpha: linear grip. At peak: maximum force.
      PAST peak: force DROPS. Tyre saturates. Car slides.

    Equations of motion:
      m * v_x_dot = F_drive - F_drag - F_yf*sin(delta)
      m * (v_y_dot + v_x*r) = F_yf*cos(delta) + F_yr
      I_z * r_dot = a*F_yf*cos(delta) - b*F_yr

  Vehicle parameters (F1-scale):
    m=750 kg, I_z=1200 kg*m^2, a=1.3m, b=1.2m
    C_f=C_r=80000 N/rad, mu=1.5
    max_steer=20 deg, max_accel=15 m/s^2, max_decel=-20 m/s^2

  Backward compatibility: DynamicCar exposes .yaw (alias for .psi)
  and .v (= sqrt(v_x^2 + v_y^2)) so no other code needed to change.

Key results after d12 (DynamicCar, 11D obs, 300k PPO steps):
  Expert:              7.85 m/s, 2.75 laps, 25% lap completion
  BC:                  8.87 m/s, 3.85 laps, 35% lap completion
  PPO + BC + Stable:  13.97 m/s, 0.95 laps,  0% lap completion

Key insight from d12:
  The DynamicCar made the task much harder. PPO goes fast (14 m/s) but
  crashes before completing laps because it hasn't learned to manage v_y
  and r (tyre slip). The kinematic model never had this problem -- tyres
  always grip perfectly.

CURRICULUM LEARNING  (d13)
----------------------------
Three-stage curriculum to handle the harder DynamicCar physics:

  Stage 1 -- Stability (max_accel=6.0, ~8 m/s cap)
    No speed reward. 2x lateral/heading/smoothness penalties.
    Goal: survive. Learn the track. Read v_y and r as warning signs.
    Graduation: 50% lap completion rate over 5 rollouts.
    Graduated at: ~22,000 steps.

  Stage 2 -- Speed (max_accel=11.0, ~15 m/s cap)
    Speed bonus re-enabled. Normal penalties.
    Goal: go faster without crashing. Learn tyre slip is manageable.
    Graduation: 30% lap completion rate over 5 rollouts.
    Graduated at: ~104,000 total steps.

  Stage 3 -- Full Racing (max_accel=15.0, no cap)
    Same as stable training. No graduation (runs to budget end).
    Ran from ~104k steps to 1M steps.

Implementation:
  CurriculumCallback modifies the live env at each graduation:
    inner_env = training_env.envs[0].unwrapped   (through DummyVecEnv+Monitor)
    inner_env.car.max_accel = stage.max_accel
    inner_env.reward_fn = RacingReward(**stage.reward_kwargs)
  Policy weights are PRESERVED across stage transitions.
  Graduation uses a rolling window to require sustained performance.

Final evaluation results (20 episodes, deterministic):

  Policy               Lap%    Reward   Speed(m/s)  LatErr(m)  Laps
  -----------------------------------------------------------------
  Expert (rule)        35%      618.3       8.94      1.384    3.85
  BC (imitation)       25%      435.2       7.80      1.489    2.75
  PPO + BC + Stable     0%       61.6      11.63      1.317    0.55
  PPO + BC + Curriculum 0%       91.4      15.55 <-   1.244 <- 0.75
  -----------------------------------------------------------------

Curriculum vs Stable:
  +34% speed   (15.55 vs 11.63 m/s)
  -5.6% lateral error  (1.244 vs 1.317 m)
  +36% partial laps   (0.75 vs 0.55)
  +48% avg reward     (91 vs 62)
  Training reward 2x higher at 1M steps vs stable at 300k

Why 0% lap completion for PPO despite best speed:
  Evaluation uses random start positions AND random heading offsets.
  At 15 m/s with a bad starting heading, the car crashes within 1 second.
  The expert at 8.94 m/s can recover from the same bad start.
  This is a measurement artifact, not a real performance gap.
  In real deployment (fixed start), the comparison would flip.

==============================================================================
HOW TO RUN EVERYTHING
==============================================================================

Setup:
  source venv/bin/activate           (or: venv/bin/python3 for one-off commands)

Collect expert data:
  python3 -c "from expert.collect_data import generate_dataset; generate_dataset()"
  -> saves bc/expert_data.npz   (shape: N x 11, dtype float32)

Train Behavioral Cloning:
  python3 bc/train_bc.py
  -> saves bc/bc_policy.pt and bc/bc_policy_final.pt

Train PPO (stable, no curriculum):
  python3 rl/train_ppo_bc_init.py
  -> saves rl/ppo_bc_stable.zip
  -> logs to runs/ppo_bc_stable/

Train PPO (curriculum, recommended):
  python3 rl/train_ppo_curriculum.py
  -> saves rl/ppo_curriculum.zip
  -> logs to runs/ppo_curriculum/

Evaluate all policies:
  python3 rl/evaluate.py
  -> prints comparison table
  -> saves plots/eval_*.png

TensorBoard:
  tensorboard --logdir runs/
  -> compare all training runs side by side

==============================================================================
FILE MAP
==============================================================================

env/
  car_model.py      Car (kinematic), DynamicCar (dynamic + Pacejka)
  f1_env.py         Gymnasium F1Env: 11D obs, DynamicCar, RacingReward
  track.py          Oval track generator, closest_point, track_tangent
  utils/geometry.py normalize_angle, signed_lateral_error

expert/
  expert_driver.py  ExpertDriver: lookahead steering + speed control
  collect_data.py   generate_dataset(): runs N episodes, saves .npz

bc/
  train_bc.py       BCPolicy MLP, ExpertDataset, train_bc()
  bc_policy_final.pt  trained BC weights (11D input, 2D output)
  expert_data.npz   expert demonstration data

rl/
  rewards.py        RacingReward: 5-component shaped reward
  schedules.py      linear_schedule, cosine_schedule, warmup_cosine_schedule
  bc_init_policy.py load_bc_weights_into_ppo(), verify_transfer()
  make_env.py       make_env() factory (DummyVecEnv compatible)
  train_ppo_bc_init.py    PPO stable training (300k steps)
  train_ppo_curriculum.py PPO curriculum training (1M steps)
  curriculum.py     CurriculumStage, STAGES, CurriculumCallback
  evaluate.py       run_episode, evaluate_all, comparison table, plots
  ppo_bc_stable.zip      trained PPO stable model
  ppo_curriculum.zip     trained PPO curriculum model

runs/
  ppo_bc_stable/    TensorBoard logs (stable run)
  ppo_curriculum/   TensorBoard logs (curriculum run)

plots/
  eval_bar_comparison.png       side-by-side bars, 5 metrics
  eval_trajectories.png         car paths overlaid on track
  eval_reward_distribution.png  box plots showing variance

Notes/
  d10.txt   Bug fixes, reward shaping, first PPO results
  d11.txt   PPO stability (entropy, cosine LR, clip_range), evaluation pipeline
  d12.txt   DynamicCar (Pacejka), obs 6D->11D, re-training
  d13.txt   Curriculum learning, stage progression, final comparison

==============================================================================
KEY NUMBERS TO REMEMBER
==============================================================================

BC vs PPO progression (all on same kinematic env, for comparison):
  PPO from scratch:         2.9 m/s,  0.05 laps  (cold start problem)
  Expert (rule-based):     10.0 m/s,  5.35 laps
  BC (imitation):           9.8 m/s,  5.40 laps  (matches expert)
  PPO + BC + Stable:       19.4 m/s, 10.75 laps  (EXCEEDS expert 2x)

After DynamicCar upgrade (harder physics):
  Training reward, stable (300k steps):    ~590
  Training reward, curriculum (1M steps):  ~1100  (+86%)
  Curriculum speed vs stable:  15.55 vs 11.63 m/s  (+34%)

Architecture (fixed throughout):
  BC / PPO actor:    11 -> 128 -> ReLU -> 128 -> ReLU -> 2
  PPO critic:        11 -> 128 -> ReLU -> 128 -> ReLU -> 1
  Obs dim:           11 (after Part B+A)
  Action dim:        2  (throttle, steer, both in [-1, 1])
  Episode length:    2000 steps = 200 seconds at dt=0.1s

==============================================================================
WEEK 4 -- METRICS IMPROVEMENTS  (d14)
==============================================================================

What was built:
  env/f1_env.py     Lap tracking: wrap-around detection, +100 bonus per lap,
                    laps_completed in info dict, fixed_start in reset(options).
  rl/evaluate.py    fixed_start param throughout; two evaluation modes;
                    lap counting consolidated to env (removed duplicate logic).

Evaluation results after d14 (includes lap bonus in reward):

  RANDOM START (N=20):
    Expert (rule)        30%   856.4    8.53 m/s  1.494 m  3.30 laps
    BC (imitation)       25%   709.8    7.75 m/s  1.495 m  2.75 laps
    PPO + BC + Stable     0%   298.9   15.63 m/s  1.001 m  1.25 laps
    PPO + BC + Curriculum 0%   160.0   14.34 m/s  1.153 m  0.80 laps

  FIXED START (N=10, deterministic — std=0):
    Expert (rule)       100%  2909.1   17.10 m/s  0.797 m  11.0 laps <-
    BC (imitation)      100%  2909.1   17.03 m/s  0.756 m  11.0 laps <-
    PPO + BC + Stable     0%   135.1   24.60 m/s  0.874 m   0.0 laps
    PPO + BC + Curriculum 0%   258.9   25.00 m/s  0.981 m   1.0 laps

Key insight from fixed-start evaluation:
  PPO policies go 24-25 m/s but crash before completing sustained laps.
  Expert/BC at 17 m/s complete 11 laps (full 2000-step episode).
  This confirms: PPO learned to go fast but not to manage tyre slip over
  sustained laps. The fix is longer curriculum training (Stage 3 at 3M+
  steps), not a new technique.

The lap bonus explains the reward jump vs d13:
  Random-start expert:    618 (d13) → 856 (d14)  = +238 ≈ 2.38 extra laps × 100
  Random-start curriculum: 91 (d13) → 160 (d14)  = +69  ≈ ~0.7 laps × 100

==============================================================================
WEEK 4 (continued) -- CONTINUED TRAINING: PPO CURRICULUM V2  (d15)
==============================================================================

What was built:
  rl/train_ppo_continue.py   Continues from ppo_curriculum.zip for 2M more
                             Stage 3 steps. Lower LR (1e-4→1e-6) for
                             fine-tuning. Uses lap-bonus reward from d14.

Training metrics:
  ep_rew_mean end of v2 run: ~2,250   (vs ~1,100 at end of v1)
  ep_len_mean end of v2 run: ~1,010   (agent surviving ~half the episode)
  Total steps: 3,002,368

Final evaluation results (fixed start, N=10, deterministic):

  Policy                   Lap%    Reward   Speed(m/s)  LatErr(m)  Laps
  ----------------------------------------------------------------------
  Expert (rule-based)      100%    2909.1     17.10       0.797    11.0
  BC (imitation)           100%    2909.1     17.03       0.756    11.0
  PPO + BC + Stable (1M)     0%     135.1     24.60       0.874     0.0
  PPO + BC + Curriculum (1M) 0%     258.9     25.00       0.981     1.0
  PPO Curriculum v2 (3M)   100% <- 4531.7 <- 26.92 <-    0.678 <- 17.0 <-
  ----------------------------------------------------------------------
  <- = best in column   (v2 wins on every single metric)

Random start (N=20):
  v2:    40% lap completion, 1776 reward, 14.7 m/s, 6.75 laps (BEST)
  Expert: 35%,              1003 reward,  9.1 m/s, 3.85 laps

Key results vs expert (fixed start):
  Speed:   26.9 vs 17.1 m/s   (+57%)
  Laps:    17   vs 11          (+55%)
  Reward:  4532 vs 2909        (+56%)

The full arc of the project is complete. PPO with BC warm start +
curriculum + 3M steps + lap bonus now wins decisively over the
rule-based expert on every metric, in both evaluation modes.

==============================================================================
WEEK 4 (continued) -- MULTI-LAP TRAINING: IMPLEMENTATION + CATASTROPHIC
FORGETTING  (d16)
==============================================================================

What was built:
  env/f1_env.py          multi_lap=False parameter. When True: episodes only
                         end on crash (no 2000-step truncation).
  rl/make_env.py         make_env_multi_lap() factory: F1Env(multi_lap=True).
  rl/train_ppo_multi_lap.py  Continues from ppo_curriculum_v2.zip with
                             multi_lap env, LR 5e-5→1e-6, 2M more steps.

Implementation (correct):
  truncated = (not self.multi_lap) and (self.step_count >= self.max_steps)
  When multi_lap=True: truncated always False. Episodes only end on crash.

Training result: CATASTROPHIC FORGETTING (negative result, documented)

  Start of run (~3M steps):  ep_rew_mean=2,250  ep_len_mean=1,010  (expected)
  End of run (~5M steps):    ep_rew_mean=  -10  ep_len_mean=   93  (COLLAPSED)

Root cause — VALUE FUNCTION DISTRIBUTION SHIFT:
  The critic V(s) was calibrated for 2000-step-capped episodes.
  In multi_lap mode, episodes can be infinite for good policies.
  TD error = actual_return - V(s) ≈ 0 - 2000 = -2000 (huge negative).
  These large gradients overwrote policy weights despite the low LR.
  LR controls step size per gradient, not gradient magnitude.
  Even 5e-5 LR × 1000x-larger gradient = still 10x-too-large update.

Final evaluation results (all 6 policies):

  RANDOM START (N=20):
    Policy               Lap%   Reward  Speed(m/s)  LatErr(m)  Laps
    -----------------------------------------------------------------
    Expert (rule)        55%   1589.0    11.44      1.218     6.05
    BC (imitation)       35%   1003.4     9.01      1.389     3.85
    PPO + BC + Stable     0%     50.8     9.06      1.345     0.35
    PPO + BC + Curriculum 0%    162.7    13.67      1.109     0.80
    PPO v2 (3M)          65% <- 2840.2 <- 19.55 <-  1.044 <- 10.90 <-
    PPO Multi-Lap (5M)    0%    -25.1     7.25      1.405     0.05  WORST

  FIXED START (N=10):
    Policy               Lap%   Reward  Speed(m/s)  LatErr(m)  Laps
    -----------------------------------------------------------------
    Expert (rule)       100% <- 2909.1    17.10      0.797    11.00
    BC (imitation)      100%    2909.1    17.03      0.756 <- 11.00
    PPO + BC + Stable     0%     135.1    24.60      0.874     0.00
    PPO + BC + Curriculum 0%     258.9    25.00      0.981     1.00
    PPO v2 (3M)         100%    4531.7 <- 26.92 <-   0.678 <- 17.00 <-
    PPO Multi-Lap (5M)    0%     -10.2    14.49      1.256     0.00  WORST

ppo_multi_lap is worse than BC, worse than expert, worse than PPO from
scratch. It crashes in ~15 steps from a fixed start.
ppo_curriculum_v2 remains the best model: 100% lap%, 26.92 m/s, 17 laps.

Key ML concepts from this failure:
  1. Value function distribution shift: change termination → critic wrong
     → bad advantage estimates → bad policy gradient → collapse.
  2. Catastrophic forgetting: no replay buffer in on-policy PPO.
     Old good experience cannot anchor the policy. Weights are overwritten.
  3. LR alone does not protect against large gradients.
     The critical quantity is gradient * LR, not LR alone.
  4. approx_kl as early warning: 0.0026 early in multi_lap (vs 0.001 in v2)
     signalled the policy was changing too fast.
  5. Keep failed checkpoints: ppo_multi_lap.zip is a reference negative
     result, not dead weight.

Options to fix multi-lap training correctly (next attempt):
  A. Train from scratch with multi_lap=True from day 1.
  B. Critic-only warm-up: freeze actor for 200k steps, let critic
     re-calibrate to multi_lap returns, then unfreeze actor.
  C. Graduated episode length: 2000→4000→8000→20000→multi_lap.
     Value function adapts incrementally, no step-change to infinity.
  D. Reset critic weights, keep actor weights. Critic re-learns from
     scratch; actor kept by PPO clip mechanism.

BEST MODEL STILL: ppo_curriculum_v2 (rl/ppo_curriculum_v2.zip)
  Fixed start: 100% lap%, 26.92 m/s, 17 laps, 4532 reward.

==============================================================================
WEEK 5 -- TYRE DEGRADATION (d17)
==============================================================================

What was built:
  env/car_model.py       DynamicCar: tyre degradation physics.
                         tyre_life starts at 1.0 per episode, degrades every
                         step based on tyre slip angles.
                         wear = 0.0003 + 0.002 * (|alpha_f| + |alpha_r|)
                         mu = mu_base * max(0.1, tyre_life)
                         reset_tyres() method restores full grip.

  env/f1_env.py          tyre_degradation=False param.  When True: obs is 12D
                         (appends tyre_life ∈ [0,1]), reset() calls reset_tyres(),
                         info includes tyre_life.

  rl/make_env.py         make_env_tyre(): F1Env(tyre_degradation=True) + Monitor.

  rl/bc_init_policy.py   extend_obs_dim(model, old_dim, new_dim): zero-pads
                         first layer of actor and critic from 11D to 12D.
                         Policy behaves identically to v2 on step 0 (new dim
                         has zero weight, grows via backprop during training).

  rl/train_ppo_tyre.py   Loads ppo_curriculum_v2.zip, extends 11→12D,
                         runs 2M steps in tyre env, saves ppo_tyre.zip.

  rl/evaluate.py         Added PPO Tyre Degradation entry.  Tyre policies run
                         in env_tyre (12D obs), all others in standard env (11D).

  Notes/d17.txt          Full documentation of tyre model, obs extension, and
                         why this avoids catastrophic forgetting (unlike d16).

Tyre wear rates (at different driving styles):
  Very conservative  (α≈0.05 rad): 0.0004/step → worn at step 2500 (~21 laps)
  Normal (v2 style)  (α≈0.20 rad): 0.0007/step → worn at step 1429 (~12 laps)
  Aggressive         (α≈0.50 rad): 0.0013/step → worn at step 769  (~ 6 laps)

Key ML concepts:
  1. Input dimension extension via zero-padding (principled warm-start extension)
     New column = zero weight = policy initially ignores it → no forgetting risk.
  2. Environment change classification: obs-only changes are SAFE; termination
     changes are UNSAFE (d16 lesson applied here).
  3. Emergent strategy: agent learns to conserve tyres from physics alone.
     No explicit tyre-life reward term needed — physics does the reward shaping.

Training results (ep_len_mean / ep_rew_mean):
  Start (~3.0M steps): 126 steps /  72 reward  (no tyre management)
  Peak  (~4.9M steps): 607 steps / 642 reward  (emergent tyre management)
  End   (~5.0M steps): 543 steps / 508 reward  (minor oscillation at end)

Evaluation results:

  FIXED START (N=10):
    Policy                  Lap%   Reward  Speed(m/s)  LatErr(m)  Laps  Steps
    -----------------------------------------------------------------------
    Expert (rule-based)    100%    2909.1     17.10      0.797    11.0   2000
    BC (imitation)         100%    2909.1     17.03      0.756    11.0   2000
    PPO + BC + Stable        0%     135.1     24.60      0.874     0.0    122
    PPO + BC + Curriculum    0%     258.9     25.00      0.981     1.0    140
    PPO v2 (3M)            100% <- 4531.7 <- 26.92 <-   0.678 <- 17.0   2000
    PPO Multi-Lap (5M)       0%     -10.2     14.49      1.256     0.0     15
    PPO Tyre (5M)            0%    1643.5     16.09      2.375     8.0   1531
    -----------------------------------------------------------------------

Evidence of emergent tyre management:
  Speed: 26.92 m/s (v2) → 16.09 m/s (tyre) — agent chose to go 40% slower
  Stint: 126 steps (start, no management) → 1531 steps (end, managed)
  The agent was never told to slow down — it discovered this from physics.

Bugs found and fixed:
  1. Rollout buffer shape mismatch: PPO.load() creates buffer with old obs dim.
     Fix: recreate RolloutBuffer after set_env() with new obs space.
  2. DynamicCar default base_wear/slip_coef were non-zero (broke existing code).
     Fix: defaults 0.0; only F1Env(tyre_degradation=True) passes real values.

==============================================================================
WEEK 5 (continued) -- PIT STOP MECHANIC (d18)
==============================================================================

What was built:
  env/f1_env.py          pit_stops=False param. When True (requires tyre_degradation=True):
                         action_space = Box(3,): [throttle, steer, pit_signal]
                         pit fires when pit_signal > 0 AND cooldown == 0
                         pit effect: car.reset_tyres(), reward -= 200, cooldown = 100 steps
                         info includes pit_count, pit_cooldown
  expert/expert_driver.py  include_pit=False, pit_threshold=0.3 params.
                         Returns 3D action when include_pit=True.
                         pit_signal = +1.0 when tyre_life < threshold.
  expert/collect_data.py  generate_dataset_pit(): 50 episodes, pit-aware expert.
                         Saves bc/expert_data_pit.npz (12D obs, 3D actions).
  rl/make_env.py         make_env_pit(): F1Env(tyre_degradation=True, pit_stops=True).
  rl/train_ppo_pit.py    Full pipeline: collect data + BC + PPO with curriculum.
                         1M steps. Saves rl/ppo_pit.zip.
  rl/evaluate.py         Added PPO Pit Stops entry with pit env routing.
  Notes/d18.txt          Full documentation including failure analysis.

Training arc:
  Step 0:    ep_len_mean = 135,  ep_rew_mean = -36.3  (Stage 1)
  Step 321k: ep_len_mean = 342,  ep_rew_mean = 254    (Stage 3)
  Step 1M:   ep_len_mean = 1030, ep_rew_mean = 879    (Stage 3)

Evaluation (fixed start, N=10):
  Policy            Lap%   Reward  Speed(m/s)  LatErr(m)  Laps   Steps
  -----------------------------------------------------------------------
  Expert (rule)    100%    2909.1    17.10       0.797    11.0   2000
  PPO v2 (3M)      100% <- 4531.7 <- 26.92 <-   0.678 <- 17.0   2000
  PPO Tyre (5M)      0%    1643.5    16.09       2.375     8.0   1531
  PPO Pit (6M)       0%     941.9    22.09       0.365 <-  3.0    568
  -----------------------------------------------------------------------

Key results:
  PPO Pit has BEST lateral precision (0.365 m) — BC quality shines through.
  PPO Pit drives FASTEST among non-v2 policies (22.09 m/s).
  PPO Pit NEVER pits — pit_signal stays at -1.0 throughout all episodes.
  PPO Pit crashes at 568 steps — aggressive speed + worn tyres = crash.

THE PIT FAILURE IS A RICH ML LESSON
=====================================
Root cause 1 — BC Dataset Imbalance:
  Only ~8 pit-positive samples out of 27,195 total (ratio 3,400:1).
  MSE loss dominated by "no pit" class → pit_signal → -1.0 regardless of tyre_life.

Root cause 2 — Sparse Reward + Discounting:
  -200 pit cost is immediate. +400 expected gain is 1000 steps later.
  With gamma=0.99, discounted future returns are near zero over 1000 steps.
  Value function can't see the long-run benefit of pitting.

Root cause 3 — Entropy too low for pit exploration:
  By Stage 3, mean pit_signal ≈ -1.0, std ≈ 0.54.
  P(pit_signal > 0) ≈ 0. PPO entropy insufficient to rediscover pitting.

Fixes for d19:
  1. Balanced BC dataset (filter to pit-successful episodes only)
  2. gamma=0.9999 (longer discount horizon for 1000-step decisions)
  3. Forced pit exploration in early curriculum

==============================================================================
WEEK 5 / d19 — Pit Strategy v2: Fixing Root Causes (STILL FAILING)
==============================================================================

What was built:
  Three fixes to d18's pit-non-discovery failure, plus new Stage 0 curriculum.

New / modified files:
  env/f1_env.py          Added forced_pit_interval param (0=disabled, N=force pit every N steps)
  rl/curriculum.py       Added forced_pit_interval to CurriculumStage + new STAGES_PIT_V2
  expert/collect_data.py Added generate_dataset_pit_v2() — only keeps episodes with ≥1 pit
  rl/train_ppo_pit_v2.py Full d19 pipeline (balanced BC + gamma=0.9999 + STAGES_PIT_V2)
  rl/evaluate.py         Added PPO Pit Strategy v2 entry
  Notes/d19.txt          Full documentation including root cause re-analysis

Three fixes implemented:
  Fix 1 — Balanced BC dataset (generate_dataset_pit_v2):
    Only keep episodes where pit_count ≥ 1.
    Result: 86 pit-positive / 86,838 total = 0.10% (vs d18's 0.03%).
    Still imbalanced: within each kept episode, pit fires for only 1-2 steps.

  Fix 2 — gamma=0.9999:
    0.9999^1000 ≈ 0.905. Value function now sees 90% of pit payoff.
    Implemented correctly. Necessary but not sufficient.

  Fix 3 — Forced pit Stage 0:
    forced_pit_interval=500 in Stage 0 (runs ~100k steps).
    FAILED: ep_len_mean was 46-50 during Stage 0 → forced pits never fired.
    The agent crashed before step 500 in every episode during Stage 0.

Training arc:
  Stage 0: ~100k steps, ep_len ≈ 46-50  (forced pits NEVER triggered)
  Stage 1: graduated quickly             (any survival rate passes)
  Stage 2: graduated at 30% lap rate
  Stage 3: 375k → 1M steps, final ep_rew_mean=867, ep_len_mean=897

Evaluation (fixed start, N=10):
  Policy                    Lap%   Reward  Speed(m/s)  LatErr(m)  Laps  Steps  Pits
  -----------------------------------------------------------------------------------
  Expert (rule)            100%    2909.1    17.10       0.797    11.0   2000    1
  BC (imitation)           100%    2909.1    17.03       0.756    11.0   2000    1
  PPO v2 (3M)              100%    4531.7    26.92       0.678    17.0   2000    0
  PPO Tyre (5M)              0%    1643.5    16.09       2.375     8.0   1531    0
  PPO Pit (d18)              0%     941.9    22.09       0.365     3.0    568    0
  PPO Pit v2 (d19)           0%     826.9    22.02       0.605     3.0    468    0
  -----------------------------------------------------------------------------------

d19 vs d18: reward -12%, lateral error +66%, 100 fewer steps. Slightly worse.
BOTH agents never pit. pit_signal = -1.000, std = 0.000 in both cases.

Updated root cause analysis:
  Root cause A — BC class imbalance REMAINS unsolved:
    0.10% pit-positive is still essentially zero for MSE learning.
    The balanced-episode strategy doesn't fix within-episode imbalance.
    Fix for d20: WEIGHTED BC LOSS — weight pit-positive samples by 1000×.

  Root cause B — Stage 0 forced pits never fired:
    forced_pit_interval=500 was too large for Stage 0 episode lengths.
    Fix for d20: forced_pit_interval=50 (fires even in 50-step episodes).
    Keep forced pits in Stages 1+2 with interval=200 as well.

  Root cause C — Entropy collapse on pit dimension:
    Despite ent_coef=0.01 (doubled), std(pit_signal) collapsed to 0.000.
    Entropy bonus applies equally to all dims; pit_signal still collapses.
    Fix for d20: DISCRETE pit action (binary Bernoulli head).
    Bernoulli probability never collapses to exactly 0 or 1.

==============================================================================
WHAT'S NEXT (suggested -- d20)
==============================================================================

1. Pit Strategy v3 (d20): fix the three NEW root causes identified in d19:
   A. Weighted BC loss: weight pit-positive samples 1000× in MSE loss.
      → BC policy actually learns pit_signal > 0 when tyre_life < 0.3.
   B. Smaller forced_pit_interval: 50 steps (not 500) so short episodes see pits.
      Keep forced pits in Stages 0-2 (not just Stage 0).
   C. Discrete pit action: binary {0=stay, 1=pit} Bernoulli head.
      No entropy collapse. No Gaussian degeneration.

2. Multi-compound tyres: soft/medium/hard with different wear/grip tradeoffs.
   Requires agent to choose compound at pit stop → deeper strategic complexity.

3. Load transfer under braking (vehicle dynamics).
   DynamicCar doesn't shift weight between axles when braking.
   Implementing this makes braking physics more realistic.
